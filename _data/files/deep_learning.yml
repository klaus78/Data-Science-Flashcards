- h2: Deep Learning
  h3: Introduction to Deep Learning
  question: What is Deep Learning? Why it is called deep?
  answer: >
    <b>Deep Learning</b> is a subset of machine learning that uses artificial neural networks to model and solve complex tasks. 
    These neural networks are composed of multiple layers (hence <b>"deep"</b>), and they are trained on large datasets to make 
    predictions or decisions. These deep NNs can lean from complex and unstructured data like texts, images or even sounds.

  question: What are the differences between machine learning and deep learning?
  answer: >
    <b>Deep learning</b> is a subset of <b>machine learning</b>. Both machine learning
    and deep learning try to create a mathematical model of a dataset. This mathematical
    model can then be used to make predictions on new input data.
    <br>
    However ML & DL differ in some points:
    <ul>
    <li><b>Techniques used</b>: machine learning can use several different techniques to create a mathematical model, 
    like regression, neural networks, nearest neighbors and decision trees.  By contrast, deep learning is
    based only on neural networks. These neural networks have a large number of hidden layers, which is what the 
    word <i>deep</i> refers to.</li>
    <li><b>Quantity of data</b>: while machine learning algorithms can also work with small datasets, deep learning requires
    very large datasets.</li>
    <li><b>Feature selection</b>: in machine learning, the user selects the most important features or creates new ones 
    for the task at hand. By contrast, in deep learning the feature selection is done automatically by the algorithms during
    the training process.</li>
    </ul>

- question: What is Deep Learning used for?
  answer: >
    Deep learning is used in a wide range of applications, including image and speech recognition, natural language processing, 
    autonomous vehicles, recommendation systems, and more.

- h3: Neural Networks
  question: What is an Artificial Neural Network (ANN)?
  answer: >
    An <b>artificial neural network</b> is a computational model inspired by the structure and functioning of the human brain. It 
    consists of interconnected nodes (neurons) organized in layers, typically an input layer, one or more hidden layers, 
    and an output layer.

- question: What are Neurons in a Neural Network?
  answer: >
    <b>Neurons (also called nodes)</b> in a neural network are basic computational units that receive inputs, perform a weighted sum 
    of those inputs, and apply an activation function to produce an output. They are the building blocks of the network.

- question: What is a Shallow Neural Network and Deep Neural Network?
  answer: > 
    A <b>shallow neural network</b> is a neural network which typically has only one or two hidden layers 
    (layers between the input layer and the output layer).
    <br><br>
    By contrast, <b>Deep neural networks</b> have multiple hidden layers. They are capable of learning complex representations, 
    whereas shallow networks are less capable of capturing intricate patterns.

- h3: Training and Optimization
  question: How are weights initialized in a neural network?
  answer: >
    There are several ways to initialize weights in a neural network. Here are some of the most common methods:
    <ul>
      <li><b>Initialize the weights to zero</b>: this makes your model similar to a linear model. All the neurons and every
    layer perform the same operation, giving the same output and making the deep net useless.</li> 
      <li><b>Initializing all weights randomly</b>: the weights are assigned randomly by initializing them very close
        to 0. It gives better accuracy to the model since every neuron performs different computations. This is the
        most commonly used method</li>
    </ul> 

- question: What is an Activation Function?
  answer: >
    An <b>activation function</b> introduces non-linearity into a neural network. It determines whether a neuron should be 
    activated (output a non-zero value) or not. Common activation functions include ReLU (Rectified Linear Unit), 
    Sigmoid, and Tanh.

- question: What is a Cost Function?
  answer: >
    A <b>cost function (or loss function)</b> is a mathematical measure that quantifies how well a neural network's predictions match 
    the actual target values. The goal in training a neural network is to minimize this function.
    <br>
    It is primarily used for two purposes:
    <ul>
    <li><b>Model Training</b>: During the training phase, it quantifies the disparity between a machine learning model's 
    predictions and the true target values, guiding the adjustment of model parameters for better predictions. </li>
    <li><b>Evaluation</b>: After training, the cost function serves as an evaluation metric, measuring how well the model 
    generalizes to new data. Lower cost function values indicate better model performance. </li>
    </ul>

- question: What is Backpropagation?
  answer: >
    <b>Backpropagation</b> is a fundamental algorithm used to train neural networks. It involves iteratively adjusting the network's 
    weights and biases to minimize the difference between the predicted output and the actual target values.

- h3: Types of Neural Networks
  question: What are some types of neural networks? What are they used for?
  answer: > 
    Some types of neural networks are:
    <ul>
    <li><b>Convolutional Neural Networks (CNNs)</b>: used for image and video analysis, object recognition, and computer 
    vision tasks.</li>
    <li><b>Recurrent Neural Networks (RNNs)</b>: ideal for sequence data, like natural language processing, time series 
    analysis, and speech recognition.</li>
    <li><b>Long Short-Term Memory (LSTM) Networks</b>: a type of RNN that's effective at learning long-term dependencies in 
    sequences.</li>
    <li><b>Gated Recurrent Unit (GRU) Networks</b>: another variant of RNNs, useful for many of the same tasks as LSTMs but 
    with lower computational complexity.</li>
    <li><b>Autoencoders</b>: often employed for dimensionality reduction, feature learning, and data denoising. </li>
    <li><b>Generative Adversarial Networks (GANs)</b>: used for generating new data samples, image-to-image translation, and 
    creating realistic images.</li>
    <li><b>Transformers</b>: ideal for natural language processing tasks, like machine translation, language understanding, 
    and text generation.</li>
    </ul>
    
- question: What is a Convolutional Neural Network (CNN)?
  answer: >
    <b>Convolutional Neural Networks (CNN)</b> are a type of neural network designed for processing grid-like data, such as 
    images and video. They use convolutional layers to automatically learn features from the input data.

- question: What is a Recurrent Neural Network (RNN)?
  answer: >
    <b>Recurrent Neural Networks (RNN)</b> are neural networks with loops that allow information to be passed from one step 
    of the network to the next. They are commonly used for sequential data, like time series and natural language processing.

- question: What kind of neural networks works particularly well with image data?
  answer: >
    <b>Convolutional neural networks</b> are a kind of neural network that works
    particularly well with image data.
    <br>
    The basic idea behind convolutional neural networks is to 
    divide the input image into small fragments and apply a number of filters on 
    each fragment. Applying a filter is equivalent to searching for a pattern in the fragment.
    Each filter has a specific task, like searching for edges or for circles.
    <br>
    In this way convolutional neural networks can better deal with images that are not very similar,
    for example because the images are rotated differently.

- question: What kind of neural network is particulary good at processing text data?
  answer: >
    A kind neural network that is particulary good at processing text data is 
    <b>recurrent neural networks</b>.
    <br>
    A recurrent neural network has an intern loop, that means the data is not processed all at once 
    but rather is processed in different steps. In particular in the case of text, each word is 
    processed once at a time. This process is similar to the way humans read text.

- h3: Transfer Learning
  question: What is Transfer Learning? and when to use it?
  answer: >
    <b>Transfer learning</b> is a technique where a pre-trained neural network is adapted for a new, related task. This can 
    significantly reduce the amount of data and training time required for the new task.
    <br>
    The most important scenarios to consider using transfer learning are:
    <ul>
    <li><b>Limited Data</b>: When you have a small dataset that is insufficient to train a deep model effectively, transfer learning 
    can provide a substantial boost in performance. </li>
    <li><b>Similar Tasks</b>: If your problem is closely related to a task for which a pre-trained model exists, transfer learning 
    is highly advantageous, as it leverages the knowledge from the related task. </li>
    <li><b>Resource Constraints</b>: When computational resources, including time and hardware, are limited, using pre-trained models 
    is efficient compared to training a model from scratch. </li>
    <li><b>Domain Adaptation</b>: When you need to work in a new domain but have access to data from a related domain, transfer 
    learning can be used to adapt the model from the related domain to the new one. </li>
    <li>You do not have enough data to train a model from scratch. Starting with a pre-trained can allow you
    to train the model for your task.</li>
    <li>You do not have enough time to train a model from scratch. Because training models can take days or even
    weeks, using a pre-trained model is much more faster.</li>
    </ul>

- question: Explain the concept of transfer learning in deep learning. Provide a real-world example.
  answer: >
    <b>Transfer learning in deep learning</b> is a technique where a pre-trained neural network, typically on a large dataset, is 
    used as a starting point to solve a different but related problem. The idea is to leverage the knowledge gained during 
    training on the initial task and adapt it to the new task, often with less training data and computation.
    <br><br>
    For example, a pre-trained convolutional neural network (CNN) that has learned to recognize a wide range of 
    objects in images can be fine-tuned for a specific image classification task, like classifying different species of flowers.
    The pre-trained model already possesses features that are generally useful for recognizing edges, shapes, and textures, 
    making it easier to adapt to the new task with a smaller dataset.

- question: What is the vanishing gradient problem in deep learning, and how can it be mitigated?
  answer: >
    The <b>vanishing gradient problem</b> is a challenge in training deep neural networks, particularly in recurrent 
    neural networks (RNNs) and deep feedforward networks. It occurs when gradients become extremely small during 
    backpropagation, causing the network's weights to update very slowly or not at all. This can lead to slow convergence 
    and difficulty in training deep models.
    <br><br>
    To mitigate the vanishing gradient problem, several techniques can be used:
    <ul>
      <li><strong>Activation Functions:</strong> replace activation functions like sigmoid with alternatives such as 
                ReLU (Rectified Linear Unit), which are less prone to vanishing gradients.</li>
      <li><strong>Weight Initialization:</strong> use appropriate weight initialization techniques like Xavier/Glorot 
                initialization to ensure that weights have reasonable initial values.</li>
      <li><strong>Batch Normalization:</strong> apply batch normalization to normalize the inputs at each layer, helping 
            gradients to flow more smoothly.</li>
      <li><strong>Gated Architectures:</strong> architectures like LSTMs and GRUs use gating mechanisms to control the flow of 
            information, reducing the vanishing gradient problem in sequential data models.</li>
      <li><strong>Gradient Clipping:</strong> clip gradients during training to prevent them from becoming excessively large 
          or small.</li>
    </ul>

- h3: Large Language Models (LLMs)
  question: What is a Large Language Model (LLM)?
  answer: >
    A <b>Large Language Model (LLM)</b> is a type of language model that can achieve advanced language understanding and generation.
    LLMs are highly efficient at capturing the complex entity relationships in the text at hand and can generate the text using the 
    semantic and syntactic of that particular language in which we wish to do so. They can perform tasks like text generation, 
    machine translation, summary writing, image generation from texts, machine coding, chat-bots, or Conversational AI.
    <br><br>
    LLMs are artificial neural networks, mainly transformers, that are trained on massive amount of data and take millions of parameters.
    As autoregressive language models, they work by taking an input text and repeatedly predicting the next token or word. 

- question: How do Large Language Models (LLMs) work?
  answer:

- question: What is pre-training and fine-tuning in the context of LLMs?
  answer:

- question: How are LLMs evaluated?
  answer:

- question: What is the role of attention mechanisms in LLMs?
  answer: 
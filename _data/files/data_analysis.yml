- h2: Data Analysis
  question: What are the two main types of data?
  answer: >
     The two main types of data are
     <br>- numeric.
     <br>- categorical. The variable can be only one of a finite set of values.
       
       
- question: You can divide numerical data into two types. What are they?
  answer: >
    The two types of numerical data are:
    <ul>
    <li><b>interval data</b>. Interval data is a data type that is measured on a scale where all 
    points are at the same distance. Interval data does not have a zero point.
    Without a zero point the concept of multiplication and division is not
    applicable and thus only sum and difference operations can be applied to interval data.
    An example of interval data is the temperature in C and F. You cannot say that 20C is twice
    as hot as 10C. This is equivalent to 50F and 68F which is clearly not as twice as hot.</li>
    <li> <b>ratio data</b>. Ratio data is similar to interval data except that it has a 
    meaningful zero value so that all the sum, subtraction, multiplication and division operations
    can be applied. Some examples of ratio data are the length and the weight.</li>
    </ul>

- question: You can divide categorical data into two types. What are they?
  answer: >
    There are two types of categorical data:
    <ul>
    <li><b>ordinal data</b> = this data has some kind of order but the difference between
    items is not important. An example of ordinal data are the months or the
    happiness state (sad, happy, very happy etc).</li>
    <li><b>nominal data</b> = data without order and without any quantitative value.
    Some examples of nominal data are all cities in the USA, the gender (male/female),
    the color hair (brown, black, blonde etc).</li>
    </ul>


- question: How to find outliers in a dataset?
  answer: >
    Here are some tools and techniques you can use to find outliers: 
    <ul>
    <li>a histogram.</li> 
    <li>a scatter plot.</li>
    <li>a boxplot.</li>
    <li>the interquartile range (IQR) and say that outliers are outside of the 1.5 IQR limit.</li>
    <li>The standard deviation and consider as outliers those points that are far more than 3 standard deviations
    from the mean.</li>
    <li>sort the data and look for unusual low or high values.
    </ul>


- h3: Distance metrices
  question: What is the hamming distance?
  answer: >
    The hamming distance is used for categorical features. If all the variables 
    are categorical then the hamming distance is defined as the number of mismatches.

- question: What is the Manhattan (or city block) distance?
  answer: >
    It is the number of horizontal and/or vertical steps to go from point p1 to point 
    p2

- question: What is the Levenshtein distance?
  answer: >
    The Levenshtein distance is the minimum number of single-character edits 
    (insertions, deletions or substitutions) to change one word into one another


- h3: Feature engineering
  question: What are the main techniques of feature engineering?
  answer: >
    The main techniques of feature engineering are
    <ul>
    <li><b>feature selection</b>. It selects a subset of features without
    transforming them</li>
    <li><b>feaure learning</b>. Use ML learning algorithms to extract the best features</li>
    <li><b>feature extraction</b>. transform the existing features into new features. This
    is usually done by dim reduction</li>
    <li><b>feature combination</b>. Combine single features into a more powerful feature</li>
     </ul>    

- question: What are the main purposes of feature selection?
  answer: >
    Reducing the number of features by feature selection is useful for
    <ul>
    <li>making models more understandable</li>
    <li>reducing the training times</li>
    <li>avoid the curse of dimensionality</li>
    <li>reduce the risk of overfitting by removing noise in the data</li>
    </ul>
- question: What are the main techniques of feature selection?
  answer: >
    The main techniques of feature selection are
    <br><br>- filter methods. Statistical techniques are used to evaluate the relationship 
    between indiviual features and the target features. This technique is completely independent 
    of the model
    <br><br>- wrapper method. This consists in training a machine learning model 
    by adding/removing features so you can evauate their effect on the model based on a 
    certain scoring
    <br><br>- embedded methods. The features are selected during the model training
    and more important features are assigned an higher rank. Examples of such
    models are decision trees and lasso regression models


- question: What are some differences between the main techniques of feature selection?
  answer: >
    Some differences between filter and wrapper methods are
    <br><br>- wrapper methods rely on machine learning models while filter methods do not. Filter
    methods rely on statistical values.
    <br><br>- filter methods can fail when there is not enough data to extract meaningful 
    statistical values
    <br><br>- wrapper methods take much more time since they involve training a number of
    machine learning models.
    <br><br>- using features extracted with wrapper methods can lead to machine learning models
    affected by overfitting since those features have been extracted using machine learning
    models.    


- question: What is the Z-score normalization (also called standardization)?
  answer: > 
    The <b>Z-score normalization</b> is a technique that scales the values so that they will have the properties
    of a standard distribution with mean <code>&mu; = 0</code>  and standard deviation <code>&sigma; = 1</code>.
    <br>The formula for the normalization is 
    <br> z = 
    <div class="frac">
      <span>x - &mu;</span>
      <span class="symbol">/</span>
      <span class="bottom">&sigma;</span>
    </div>
    

- question: What is the min-max scaling (also called normalization)?
  answer: >
    It is a technique that scales the values between 0 and 1. 
    <br>The formula for the min max scaling is 
    <br> 
    value_scaled = 
    <div class="frac">
      <span>value_to_scale - min</span>
      <span class="symbol">/</span>
      <span class="bottom">max - min</span>
    </div>

- question: When is Z-score normalization used? When is min-max scaling used instead?
  answer: >
    The <b>Z-score normalization</b> is used when the data has a gaussian distribution.
    <br><br>
    The <b>min-max scaling</b> is more used when the distribution is not gaussian or not known.
    In addition, the min-max scaling works better than the Z-normalization when the 
    standard deviation is very small. Indeed in such case the effect of scaling the data
    between 0 and 1 will not affect the standard deviation significantly since it 
    is already small.

- question: >
    What is the disadvantage of min-max scaling compared to Z-score normalization 
    concerning outliers?
  answer: >
    The min-max scaling is more sensitive to outliers since the data is
    bounded between 0 and 1 which automatically leads to information loss about
    outliers. 
    <br>
    The Z-score normalization is more suited when there are outliers since the
    data is not bounded but simply rescaled to have mean 0 and standard deviation 1.

- question: >
    What kind of algorithms are not affected by feature scaling? 
    Can you make some examples of such algorithms? 
  answer: >
    Any algorithm that is not distance based is not affected by feature scaling.
    <br>
    Examples of algorithms not affected by feature scaling are Naive Bayes, Tree-based
    algorithms and Linear Discriminant Analysis.

- question: How to treat missing values?
  answer: >
    There are several possible approaches to treat missing values:
    <ul>
    <li>remove items with missing values</li>
    <li>replace the missing values</li>
    </ul>

- question: When is it reasonably possible to drop items with missing values?
  answer: When the items to remove are few compared to the size of the training set 

- question: What are the common techniques for replacing missing values?
  answer: >
    When values are missing randomly you can:
    <ul>
    <li>replace the missing values with the mean</li>
    <li>interpolate values</li>
    <li>take the previous or next value</li>
    </ul>

- h3: A/B Testing
  question: What is A/B testing?
  answer: 
    <b>A/B testing</b> is a method of testing two versions of a product, for example a website or an app, to see which version performs better. In an A/B test, 
    two versions of the products are randomly assigned to users, and the performance of the two versions is compared.
    <br><br>
    A/B testing is a valuable tool for optimizing the performance of websites and apps. It can be used to test different 
    designs, features, and content to see what works best for users.

- h3: Correlation
  question: What is correlation? Is correlation related with causation? What is the difference between them?
  answer: >
    <b>Correlation</b> is a measure of the strength and direction of the relationship between two variables. 
    <br><br>
    <b>Causation</b> is the relationship between two variables where one variable causes the other variable to change.
    <br><br>
    Just because two variables are correlated does not mean that there is a causal relationship between them. 
    For example, there is a correlation between the number of ice creams sold and the number of shark attacks. 
    However, this does not mean that eating ice cream causes shark attacks.

- question: How to determine if there is a causal relationhip between two variables?
  answer: >
    To determine whether there is a causal relationship between two variables, it is necessary to conduct a 
    controlled experiment. In a controlled experiment, the researcher manipulates one variable (the independent variable) 
    and observes the effect on the other variable (the dependent variable). If the dependent variable changes when the 
    independent variable is changed, then the researcher can conclude that there is a causal relationship between the 
    two variables.

- question: What range of values can have correlation? What values are significant?
  answer: >
    Correlation is a statistical value that quantifies the dependency between
    two variables.  The correlation value ranges from -1 to +1. The -1 (+1) value represents a 
    perfectly negative (positive) correlation. The close the value to 0 is, the weakest the
    linear correlation is. As a thumb rule you can consider values < -0.5 or > +0.5
    to have a significant relationship.    
   
   
- question: There are three main types of correlation, what are they and when are they used?
  answer: >
    The three most common type of correlation are
    <br><br>- Pearson. It is the most common correlation and is used only with numerica data
    It measures the linear relationship between two variables. It also assumes
    that the variables are normally distributed.
    <br><br>- Kendall. It is a rank correlation measure. It works with interval,
    ratio and ordinal data. It measures how similar ranked orderings are. 
    No linear correlation is assumed.
    <br><br>- Spearman. It is is a rank correlation measure. No linearity is assumed but 
    a monotonic relationship, i.e. an increasing (var1 increases and also var2 increases) 
    or decreasing (var1 decreases and also var2 decreases) relationship.


- question: How can you use correlation for feature selection?
  answer: >
    There are at least two approaches:
    <br>- use the the correlation of each feature with the target. You can keep those features 
    that have a significant correlation, i.e. a correlation
    value < -0.5 or > +0.5. 
    <br>- You can use the pairwise correlation between features. If two features are highly 
    correlated then you can drop one of them since they provide the same information.

- question: How can you use variance for feature selection?
  answer: >
    Features with a low variance can be dropped since (almost) constant features do not provide
    any information.

- h3: Dimensionality reduction
  question: What is dimensionality reduction?
  answer: >
    <b>Dimensionality reduction</b> is a technique employed to decrease the number of features in a dataset 
    while retaining as much relevant information as possible. 
    <br><br>
    The primary goal of dimensionality reduction 
    is to produce a simplified and more compact representation of the data. By reducing the dimensionality of a 
    dataset, it addresses various challenges associated with high-dimensional data, including the curse of 
    dimensionality, computational complexity, and overfitting in machine learning models.
    
- question: What are the main types of dimensional reduction? 
  answer: >
    The main types of dimensionality reduction are:
    <ul>
    <li><b>Linear Dimensionality Reduction</b>: These techniques focus on linear transformations to reduce dimensionality while 
    preserving the most important information in the data. Examples include Principal Component Analysis (PCA) and Linear 
    Discriminant Analysis (LDA).</li>
    <li><b>Nonlinear Dimensionality Reduction</b>: Nonlinear techniques are used when the relationships between variables are 
    not well captured by linear methods. They aim to preserve the inherent structure in the data without imposing a linear constraint. 
    Examples include t-Distributed Stochastic Neighbor Embedding (t-SNE), Isomap, and Locally Linear Embedding (LLE).</li>
    <li><b>Neural Network-Based Dimensionality Reduction</b>: These methods use neural networks, specifically autoencoders, to learn 
    reduced-dimensional representations of the data. Autoencoders can be adapted for both linear and nonlinear dimensionality reduction.</li>
    <li><b>Sparse Dimensionality Reduction</b>: Sparse coding techniques aim to represent data points using a sparse combination of 
    basis vectors, effectively reducing dimensionality while maintaining a sparse representation.</li>
    <li><b>Manifold Learning</b>: Manifold learning techniques focus on modeling the underlying structure of data as a 
    lower-dimensional manifold embedded within a high-dimensional space. Examples include Isomap, LLE, and spectral embedding.</li>
    <li><b>Feature Selection</b>: Instead of transforming the data, feature selection techniques choose a subset of the most 
    informative features to reduce dimensionality. This can be done through filter, wrapper, or embedded methods.</li>
    </ul>

- question: What are some common algorithms of dimensionality reduction?
  answer: >
    Some common algorithms of dimensionality reduction are:
    <ul>
        <li><strong>Principal Component Analysis (PCA):</strong> PCA is a linear technique that reduces the dimensionality of data by transforming it into a new coordinate system based on the principal components, which are orthogonal and capture the most variance in the data.</li>
        <li><strong>t-Distributed Stochastic Neighbor Embedding (t-SNE):</strong> t-SNE is a non-linear technique used for visualization and dimensionality reduction by preserving pairwise similarities between data points, making it suitable for exploring high-dimensional data.</li>
        <li><strong>Linear Discriminant Analysis (LDA):</strong> LDA is a supervised technique that aims to maximize the separation between classes by projecting data points onto a lower-dimensional space.</li>
        <li><strong>Autoencoders:</strong> Autoencoders are neural network-based models that can learn efficient representations of data by encoding it into a lower-dimensional space and then decoding it back to the original dimension. Variational Autoencoders (VAEs) are a popular variant.</li>
        <li><strong>Isomap:</strong> Isomap is a non-linear technique that computes a lower-dimensional representation of data while preserving geodesic distances between data points in a manifold, which is useful for data with non-linear structures.</li>
        <li><strong>Locally Linear Embedding (LLE):</strong> LLE is another non-linear method that preserves local relationships between data points, making it suitable for capturing the intrinsic structure of data.</li>
    </ul>
    

- question: What is PCA? When do you use it?
  answer: >
    <b>Principal component analysis (PCA)</b> is a statistical method used in Machine Learning. It consists in
    projecting data in a higher dimensional space into a lower dimensional space by maximizing
    the variance of each dimension.
    <br><br>
    PCA is mostly used as a tool in exploratory data analysis and for making predictive models. It is often used
    to visualize genetic distance and relatedness between populations.

- question: >
    PCA is affected by scale. To get the optimal performance out of PCA, 
    which is a crucial first step?
  answer: >
    For optimal performance in Principal Component Analysis (PCA), a crucial first step is <strong>standardization or scaling of the data</strong>. 
    This is essential because PCA is affected by the scale of the input features.
    <p>Standardizing the data involves transforming each feature to have a mean of 0 and a standard deviation of 1. This ensures that all features are on the same scale and prevents features with larger variances from dominating the PCA results. Standardization is typically performed using the following formula for each feature:</p>
    <p><em>Standardized Value = (Original Value - Mean) / Standard Deviation</em></p>

- h3: Text processing
  question: What are some steps involved in the preprocessing of a text?
  answer: >
    The preprocessing of a text can involve
    <ul>
    <li>make text lower case</li>
    <li>remove punctuation</li>
    <li>tokenize the text (split up the words in a sentence)</li>
    <li>remove stop words as they convey grammar rather than meaning</li>
    <li>word stemming (reduce words to their stems)</li>
    </ul>
- question: What is a word cloud?
  answer: >
    The word cloud is a visualization technique for text processing where the most frequent
    words in a text are shown in bigger. 
    <br><br>
    The word cloud is not a really scientific method but it can be very useful to capture the 
    attention of people, for example in presentations. An example of word cloud is
  image: word_cloud.png

- h3: Charts
  question: What is a pie chart? When should it be used / not used?
  answer: >
    A <b>pie chart</b> is basically a circle that is divided into slices. Each slice represents a group.
    The size of each slice is proportional to the numbers of items of that group. 
    For example in the picture below, given a set of different fruits,
    you can see the proportion of each type of fruit. 
    <br><br>
    Pie charts are good at showing the relative quantities of items in an intuitive manner. In particular, pie chars
    are good at showing what groups are more or less predominant. In the picture apples are the predominant fruit.
    <br><br>
    Pie charts should not be used for exact quantitative comparisons of slices, in particular when slices have a similar size.
    Indeed it is hard for the human eye to compare areas. For example, considering the picture, pie charts should not be used 
    to find how much the cherries slide is bigger/smaller than the bananas slice.
  image: pie_chart.png

- question: What is a bar chart? When should it be used?
  answer: >
    A <b>bar chart</b> is a chart that represents categorical data with rectangular bars. The size of each bar is proportional
    to the value represented.
    <br><br>
    A bar chart is useful when you want to compare values. In particular a bar chart can also be used to compare
    values that are pretty similar. In this case a bar chart is a better choice than a pie chart because 
    pie charts are not particularly suitable for comparing similar values. 
  image: bar_chart.png


- question:  What are some difference between a bar chart and a histogram?
  answer: >
    A <b>bar chart</b> 
    <ul>
    <li>shows the frequency of each value of a <u>categorical</u> variable.</li>
    <li>each column represents a single value.</li> 
    <li>there are gaps between columns.</li>
    </ul>
    An <b>histogram</b> 
    <ul>
    <li>is used to represent the distribution of a <u>continuous</u> variable.</li>
    <li>each column represents a range of values.</li>
    <li>there are no gaps between columns.</li>
    <li>the area of each column is proportional to the frequency.</li>
    </ul>
  image: bar_chart_histogram.png

- question: What is a histogram particularly good at?
  answer: >
    A <b>histogram</b> is particularly good at representing grouped data. The frequency of each group is 
    represented by the area of the column. Because the width of the columns is proportional to the width of 
    the groups we can have columns with different column widths. The height of the column is then given by
    frequency / width.

- question: Can you draw a boxplot and explain it?
  answer: >
    A <b>boxplot</b> is a chart specialized in visualizing and comparing ranges.

    <br><br>The elements of a boxplot are:
    <ul>
    <li><i>upper extreme (95th Percentile)</i> = exactly 5 percent of the values in the data are 
    greater than this value (may instead be the highest value if not displayed separately as dots).</li>
    <li>
    <i>lower extreme (5th Percentile)</i> = exactly 5 percent of the values in the data are less than this value 
    (may instead be the smallest value if not displayed separately as dots).
    </li>
    <li><i>lower quartile</i> = lowest value of all quartiles. Quartiles are a set of three points that divide the dataset 
    into four equal parts.</li>
    <li><i>upper quartile</i> = highest value of all quartiles</li>
    <li><i>median</i> = middle value of the sorted dataset</li>
    </ul>
  image: boxplot.png
  
- question: What is a heatmap? When is it used?
  answer: >
    A <b>heatmap</b> is a two-dimensional grid that is used for visualizing numerical data organized in a
    table-like format.
    <br><br> 
    Each cell of the heatmap corresponds to a specific row and column in the data, while the cell color
    represents the magnitude of the corresponding value. As a result, different values are mapped to different 
    colors, allowing to identify similarities in the data.
    <br><br>
    Heatmaps are used to explore relationships and patterns in datasets. Some common uses are
    <ul>
      <li>Correlation Analysis: positive and negative correlations between variables are visualized as
      different colors and thus easy to recognize.</li>
      <li>Confusion Matrix Analysis: the performance of classification models can be put in matrix form
      and displayed by heatmaps.</li>
      <li>Genome Analysis: heatmaps are widely used in genomics to visualize gene expression levels.</li>
    </ul> 
    <br>
    An example of table-like dataset and respective heatmap is shown below: 
  image: heatmap.png

- question: What is a violin plot? When is it particularly useful?
  answer: >
    A <b>violin plot</b> is a statistical data visualization that can be thought of as an extension
    of a boxplot. To be more precise, a violin plot includes all the information contained in a boxplot 
    with the addition of a kernel density plot.
    <br><br>
    While a boxplot shows statistical data such as mean, median and interquartile ranges, a violin plot 
    also includes the data distribution. 
    <br><br>
    A violin plot is particularly useful in the following scenarios:
    <ul>
    <li><b>comparing distributions</b>: by placing the distributions side by side, it is easier
    to compare distributions, allowing for a better exploratory analysis.</li>
    <li><b>multiple modes, skewness or outliers in the data</b>: detecting such elements gives you a
    better understanding of the data.</li>
    </ul>
    A violin plot is shown in the following image. You can see the distributions of the average salaries
    for engineers and workers in a violin shape. 
    The three dotted lines in a violin plot represent the lower quartile, the median, and the upper quartile of the data set. 
  image: violin_plot.png





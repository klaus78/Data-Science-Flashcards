- h2: Machine Learning
  h3: Machine Learning basic concepts
  question: What is the difference between data science and machine learning?
  answer: >
    <b>Data science</b> and <b>Machine learning</b> are closely related fields but they differ
    in their goals.
    <br><br> 
    <b>Data science</b> is more focused on the gathering and analysis of data. Data science
    includes gathering data from different sources, cleaning the data and analyzing the data
    in order to extract useful information. 
    <br><br>
    <b>Machine learning</b> is more focused on creating a mathematical model from a dataset in order to 
    predict something. For example given a dataset of house prices, you would like to predict 
    how the house prices evolves in the next months based on the historical data in the dataset. With
    machine learning algorithms you can create mathematical models that can make such a prediction.

- question: What are the most common types of problems that machine learning can help solve?
  answer: >
    Machine learning can help solve a large number of problems which can be grouped into
    the following types:
    <ul>
    <li><b>Classification</b>: the goal of classification is to predict the category of an input item. Examples
    of classification are email spam detection and desease diagnosis.</li>
    <li><b>Regression</b>: the goal of regression is the prediction of a numerical value. Examples of
    regression are predicting the price of houses or the future demand for products.</li>
    <li><b>Clustering</b>: the goal of clustering is to group items in a way that similar items are within
    the same group. Examples of clustering are grouping customers based on purchasing behavior or grouping documents by topic.</li>
    <li><b>Recommendation</b>: the goal of recommendation is to provide personalized suggestions. Examples
    of recommendation are movie recommendation and personal marketing based on personal preferencies.</li>
    <li><b>Anomality detection</b>: the goal of anomality detection is to identify unusual patterns in the data.
    Examples of anomality detection are credit fraud detection and intrusion detection in computer networks.</li>
    <li><b>Time series analysis</b>: the goal of time series analysis is to analyze data points collected at
    specific time intervals. Examples of time series analysis are energy consumption prediction and weather forecasting.</li>
    </ul>

- question: What are the typical steps of the creation of a machine learning model?
  answer: > 
    The typical steps of a machine learning process are:
    <ul>
    <li><b>Problem definition and domain knowledge</b>: understand what problem you want to solve and gain 
    domain knowledge related to that problem.
    </li>
    <li><b>Data preparation</b>: gather, clean the data (es. handling missing values and outliers) and perform feature engineering 
    (es. select features or create new ones to be used in a machine learning algorithm).</li>
    <li><b>Data splitting</b>: split the dataset into a training dataset and a test dataset. The machine learning model
    will be trained on the training dataset and validated on the test dataset.</li>
    <li><b>Model selection</b>: select an appropriate machine learning algorithm depending on the problem at hand.</li>
    <li><b>Training</b>: train the selected machine learning algorithm on the training dataset. During the training
    the machine learning algorithm learns structures and relationships contained in the training data.</li>
    <li><b>Evaluation</b>: evaluate the performance of the trained model using the test dataset. The model
    makes predictions using the items in the test dataset and the predicted values are compared with the real values.</li>
    <li><b>Optimization</b>: adjust and tweak the trained model to improve the performance.</li>
    <li><b>Deployment</b>: the machine learning model is put into production where it can be used to make
    predictions using new data.</li>
    <li><b>Monitoring performance</b>: monitor the performance of the model over time and collect feedback. If necessary
    retrain and update the model.</li>
    </ul>
- question: >
    All machine learning algorithms work essentially in a similar way. What is it?
  answer: >
    All machine learning algorithms work essentially like
    <br>- make a prediction
    <br>- compute the error given by (predicted values - real values)
    <br>- make a new prediction that reduces the error above
    <br>- repeat until the error is minimized

  question: What is bias in a machine learning model?
  answer: >
    <b>Bias</b> is the error that is introduced by analyzing a complex model using a too simple model. 
    In other words bias is the inability of the model to capture the real nature of the true 
    relationship in the data.

- question: What does high bias mean for a machine learning model?
  answer: High bias means the model is underfitting the data.

- question: What is the variance of a machine learning model?
  answer: > 
    <b>Variance</b> is the amount that the estimate of the target function will change if different training 
    data was used.

- question: What does high variance mean for a machine learning model?
  answer: > 
    High variance means the model is overfitting, i.e. small changes in the training data set lead 
    to big changes in the output of the model.


- question: What is overfitting and what are some techniques for preventing it?
  answer: >
    Overfitting happens when your model works well with the training dataset but
    does not work well with new data, which means the model cannot generalize well. 
    <br>
    To prevent overfitting you can for example use regularization and cross-validation.

- question: What is regularization?
  answer: >
    <b>Regularization</b> adds a penalty on the different parameters of the model to reduce the 
    freedom of the model. Regularization is usually done by adding a correction term to 
    the formula for a mathematical model. This correction term penalizes the higher order 
    terms so that the model is forced to be simpler.

- question: What is the curse of dimensionality?
  answer: >
    The <b>curse of dimensionality</b> is a tendency that it is easier to overfit a dataset 
    when there are few points and many features. Data needs to increase exponentially 
    with the number of features in order not to have overfitting.

- question: What is unsupervised learning? Make also some examples of algorithms
  answer: > 
    <b>Unsupervised learning</b> is a machine learning process with unlabeled data.
    <br><br>
    Some examples of unsupervised learning are
    <br> - Clustering algorithms (K-means, hierarchical custering, Probabilistic clustering)
    <br> - Dimension reduction algorithms (PCA, Single Value Decomposition SVD)

- question: What is reinforcement learning?
  answer: >
    <b>Reinforcement learning</b> is a machine learning algorithm that receives feedback on its output so that the accuracy
    of the output is improved based on this feedback. In other words the algorithm learns through trial
    and error.

- question: What is supervised learning?
  answer: <b>Supervised learning</b> is a machine learning process with a labeled training dataset.

- question: What are the two main types of supervised learning algorithms?
  answer: >
    Supervised machine learning algorithms are mainly classified 
    into classification and regression.

- question: Can you mention some supervised machine learning algorithms?
  answer: >
    The most common <b>supervised machine learning algorithms</b> are 
    <br>K-nearest neighbor, Naive Bayes, Decision Trees, Linear Regression, 
    Support Vector Machines.

- question: What are hyperparameters?
  answer: >
    <b>Hyperparameters</b> are parameters used by the training model and 
    they must be obviously set before starting the training process. 
    An example is the the k value in the k-nearest neighbor algorithm.

- question: What are ensemble methods in machine learning?
  answer: >
    <b>Ensemble methods</b> are techniques that combine the results of a number of machine learning models rather
    than using a single model. The basic idea behind ensemble methods is than the combination of the results of a number of models 
    should be more accurate than using a single model.
    <br><br>
    To provide an example in real life, ensemble methods can be compared to asking several doctors for a diagnosis 
    rather than relying on a single doctor. In the former case, one would expect the diagnosis to be more accurate.

    
- h3: Evaluate machine learning methods
  question: To evaluate a generic machine learning model what metrics do you typically use?
  answer: > 
    The metrics to use for the evaluation of a machine learning model depend on the type of problem
    we are trying to solve.



    You would probably use the misclassification error or accuracy. 
    However, notice that accuracy works well when all classes are equally important and the 
    training dataset is balanced.	

  question: What is cross-validation in machine learning?
  answer: > 
    <b>Cross-validation</b> is a technique used in machine learning to evaluate the performance
    of a model. Cross-validation involves splitting a dataset into a 
    training set (usually 80% of the data) and test set (usually 20% of the data), training the model
    on the training set and evaluating it on the test set. 
    <br><br>
    The process is repeated multiple times
    using different splits and the result is the average evaluation over all the iterations.

- question: What is K-fold cross-validation?
  answer: >
    <b>K-fold cross-validation</b> is a specific type of cross-validation where the dataset is split into <i>K</i> equal parts. 
    <br>The steps of <b>K-fold cross-validation</b> are:
    <ul> 
    <li>split a dataset into <i>K</i> equal parts. Each part is called fold.</li>
    <li>take one fold <i>i</i> as the test set and take the remaining <i>(K-1)</i> folds as the training set.</li>
    <li>train you classification algorithm on the training set and evaluate its performance of the test set.</li> 
    <li>Repeat the process for each fold. As a result, for each fold we get an accuracy value and 
    in total we get <i>K</i> accuracy values.</li>
    <li>the final accuracy value for your classification algorithm is given by the average of
    the <i>K</i> accuracy values.</li>
    </ul>

- question: What matrix can you use to evaluate the performance of a classifier? How does an ideal matrix look like?
  answer: >
    Let's suppose we have a binary classifier that can assign an item to two
    possible classes, which are typically labeled as positive (1) and negative (0). 
    <br><br>
    To evaluate the performance of the classifier you can use the <b>confusion matrix</b>. 
    The confusion matrix displays the number of true positives (TP), true negatives (TN),
    false positives (FP) and false negatives (FN). 
    <br>
    <ul>
    <li>True positives (TP) = number of items with label positive (1) that 
    are classified correctly as positive.</li>
    <li>True negatives (TN) = number of items with label negative (0) that 
    are classified correctly as negative.</li>
    <li>False positives (FP) = number of items with label negative (0) that
    are classified wrongly as positive.</li>
    <li>False negatives (FN) = number of items with label positive (1) that
    are classified wrongly as negative.</li>
    </ul>
    <br>
    The confusion matrix looks like

    <br><br>
    <table>
      <tr>
        <td></td>
        <th><b>actual 1</b></th> 
        <th><b>actual 0</b></th>
      </tr>
      <tr>
        <td><b>predicted 1</b></td>
        <td class="true-positive">TP</td> 
        <td class="false-positive">FP</td>
      </tr>
      <tr>
        <td><b>predicted 0</b></td>
        <td class="false-negative">FN</td> 
        <td class="true-negative">TN</td>
      </tr>
    </table>
    <br><br>
    An ideal confusion matrix has high values of TP and TN because these quantities indicate the correctly
    classified items.

- question: Why is the confusion matrix called like that?
  answer: >
    The <b>confusion matrix</b> is a table that is used to evaluate the performance of a classifier.
    It displays the number of correctly classified items (true positives and true negatives) and
    wrongly classified items (false positives and false negatives).
    <br><br>
    The <b>confusion matrix</b> is called like that because it helps you identify where the 
    classifier has made wrong predictions, which is where the classifier was <b><i>confused</i></b>.
    As a result you can gain insight about the performance of the classifier and take corrective
    action to minimize the number of wrongly classified items.
    

- question: Can you mention some of the most common metrics used to evaluate binary classifiers?
  answer: >
    A binary classifier is a machine learning model that assigns an item to one of two possible classes.
    The two classes are typically referred to as the positive class and the negative class. 
    <br>
    In order to evaluate the performance of binary classifiers, the following terms are used: 
    <ul>
    <li>True positives (TP) = number of the positive class items that 
    are classified correctly as positive.</li>
    <li>True negatives (TN) = number of the negative class items that 
    are classified correctly as negative.</li>
    <li>False positives (FP) = number the negative class items that
    are classified wrongly as positive.</li>
    <li>False negatives (FN) = number of the positive class items that are classified 
    wrongly as negative.</li>
    </ul>
    <br>
    Now that we have defined TP, TN, FP and FN and given <code>n</code> as the total number of items,
    we can introduce some of the most common metrics for evaluating binary classifiers. 
    <ul>
    <li><b>Accuracy</b> is the percentage of the correctly classified items. 
    <br>Accuracy = 
      <div class="frac">
        <span>TP + TN</span>
        <span class="symbol">/</span>
        <span class="bottom">n</span>
      </div>
    </li>
    <li><b>true positive rate or sensitivity  or recall</b> is the percentage of the positive class items that
    are actually classified as positive.
    <br>Sensitivity = 
      <div class="frac">
        <span>TP</span>
        <span class="symbol">/</span>
        <span class="bottom">TP + FN</span>
      </div>
    </li>
    <li><b>true negative rate or specificity</b> is the percentage of the negative class items that
    are actually classified as negative. 
    <br>Specificity = 
      <div class="frac">
        <span>TN</span>
        <span class="symbol">/</span>
        <span class="bottom">TN + FP</span>
      </div>
    </li>
    <li><b>Precision</b> is the percentage of the items classified as positive that actually belong to the positive class. 
    <br>Precision = 
      <div class="frac">
        <span>TP</span>
        <span class="symbol">/</span>
        <span class="bottom">TP + FP</span>
      </div>
    </li>
    <li><b>False positive rate (FPR)</b> represents the proportion of negative class items 
    that are incorrectly classified as positive. 
      <br>FPR = 
      <div class="frac">
        <span>FP</span>
        <span class="symbol">/</span>
        <span class="bottom">TN + FP</span>
      </div>
    <li><b>False negative rate (FNR)</b> represents the proportion of positive class items 
    that are incorrectly classified as negative.
      <br>FNR = 
      <div class="frac">
        <span>TN</span>
        <span class="symbol">/</span>
        <span class="bottom">TP + FN</span>
      </div>
    </li>
    <ul>

- question: > 
    What metrics are more meaningful with class-balanced datasets and what work better 
    for class-unbalanced datasets? 
  answer: >
    In a class-balanced dataset, the number of items in each class is approximately the same.
    
    <br><br>When the dataset is class-balanced, <b>accuracy</b> is a good metric. Indeed, accuracy assigns
    equal weight to both true positives and true negatives. Accuracy is calculated as the ratio of correctly 
    classified instances (both true positives and true negatives) to the total number of instances, 
    as indicated by the formula
    <br>Accuracy = 
      <div class="frac">
        <span>TP + TN</span>
        <span class="symbol">/</span>
        <span class="bottom">n</span>
      </div>
    
    <br><br>When the dataset is class-unbalanced, one class has significantly more items than the
    other classes. In that case <b>precision</b> and <b>recall</b>
    are better metrices. In class-imbalanced scenarios, 
    the class with fewer instances is often the class of more significance. 
    For example, in fraud detection, the number of actual fraud cases is much lower than non-fraud cases, 
    but correctly identifying fraud is crucial. 
    <br>Let TP represent the number of correctly identified frauds. 
    Precision and recall provide a better indication of the model's ability to 
    identify the minority class accurately, as they have TP in the numerator as follows
    <br>Precision = 
      <div class="frac">
        <span>TP</span>
        <span class="symbol">/</span>
        <span class="bottom">TP + FP</span>
      </div> 
    <br>Recall = 
      <div class="frac">
        <span>TP</span>
        <span class="symbol">/</span>
        <span class="bottom">TP + FN</span>
      </div>
    <br><br>By focusing on the true positives, precision and recall provide 
    insights into how well the model performs in the context that matters most.
    <br><br>
    Precision: High precision indicates that when the model predicts the positive class, it is often correct. 
    This is important when misclassifying positive instances (false positives) could have significant consequences. 
    Precision tells you the proportion of correctly identified positive predictions out of all predicted positive instances.
    <br><br>
    Recall: High recall indicates that the model is effective at capturing most of the actual positive instances. 
    This is crucial when you want to make sure you don't miss many of the positive instances. 
    Recall tells you the proportion of correctly identified positive predictions out of all actual positive instances.
    <br><br>
    However, it's important to remember that precision and recall are trade-offs: increasing one can sometimes 
    lead to a decrease in the other. Therefore, you should choose the metric that aligns with your specific goals 
    and priorities in your application.

- question: > 
    To evaluate a machine learning model for cancer detection what metrics do 
    doctors typically use?
  answer: > 
    Doctors would use sensitivity and specificity because the cost of a misclassified cancer 
    is much higher than the cost of a healthy tissue classified as cancer. In addition a typical
    cancer dataset is strongly unbalanced, with few cancel samples and many healthy samples.

- question: What is the meaning and use of the F-measure (also called F-score)?
  answer: >
    The <b>F-measure</b> combines the precision and recall using the harmonic mean.
    <br><br>
    F-measure = 2 (Precision X Recall) / (Precision + Recall)
    <br><br>
    The F-measure value is between 0 (worst case) and 1 (best case). It can be used
    when you want to compare the performance of two classifiers w.r.t. precision and recall
    but you are not sure which of the two values to pick. So with the F-measure you seek
    a balance between precision and recall.

- question: If you want to evaluate the performance of a classifier with a curve what do you use?
  answer: >
    To evaluate the performance of a classifier you can use the <b>ROC curve</b> 
    (Receiver Operating Characteristic). The ROC curve plots the <b>sensitivity</b> (= True Positive Rate) against
     <b>1-specificity</b> (= False Positive Rate) for different values of a parameter that affects the classifier.
    <br><br>
    Sensitivity = probability of predicting that a real positive will be positive.
    <br>
    Specificity = probability of predicting that a real negative will be negative.
    <br><br>
    The best result for a classifier is high sensitivity and low 1-specificity, which means the closest the curve
    is to point (0,1) (top-left), the better. This can be also expressed as the farther the curve is
    from the diagonal line (0,0)-(1,1) the better. This diagonal line represents the line of 
    a classifier working at random. 
    <br>
    The distance of the curve from point (0,1) can be measured with the AUC (area under the curve)
    that should be close to 1 as much as possible.
  image: roc_curve.png

- h3: Naive Bayes
  question: What is Naive Bayes?
  answer: >
    Naive Bayes is a supervised machine learning algorithm based on the Bayes theorem that is used
    to solve classification problems.

- question: Why is Naive Bayes called "naive"?
  answer: >
    Because the Naive Bayes is based on the idea that the predictor variables are independent of 
    each other while in nature this is often not the case.
    <br><br>
    A Naive Bayes classifier calculates the probability of an item of belonging to each possible
    output class. The class with the highest probability is then chosen as output.

- question: What is the Bayes theorem?
  answer: >
    The Bayes theorem is a mathematical formula for finding P(A|B) from P(B|A).
    The formula is 
    <br><br>
    P(A|B) = P(B|A) * P(A) / P(B)


- question: What are the main advantages of Naive Bayes?
  answer: >
    The Naive Bayes is a relatively simple and very quick algorithm since it is a probabilistic 
    model that does not require a training step. This makes it very scalable.

- question: When is the Naive Bayes usually used?
  answer: > 
    Naive Bayes is often used in <br>
    - text classification like spam filter<br>
    - real-time classification since it is fast<br>
    - multi-class prediction

- question: When Naive Bayes is used with numerical variables, what condition is assumed on the data?
  answer: >
    Numerical variables used in Naive Bayes are expected to have a normal distribution.
    This is not always the case in practice.

- question: How does the Naive Bayes perform with categorical variables?
  answer: > 
    It performs well with categorical variables since no assumpion is made on
    the data distribution. By contrast with numerical variables a normal distribution
    is assumed.

- h3: Regression
  question: What do you predict with linear regression?
  answer: Linear regression predicts a real value

- question: What are the most common techniques used for computing the coefficients in linear regression?
  answer: >
    The most common techniques used for computing the coefficients in linear regression are 
    <br><br> - <b>Ordinary Least Squares</b> seeks to minimize the sum of the squared residuals, i.e. the distance 
    between the points and the regression line.
    <br><br> - <b>Gradient descendent</b> works by inizializing randomly the coefficients. A learning rate is 
    used as a scale factor and the coefficients are updated in the direction towards minimizing the error. 
    The process is repeated until a minimum sum squared error is achieved or no further improvement is 
    possible.

- question: What is logistic regression?
  answer: >
    <b>Logistic regression</b> is a statistical technique that is used to analyze a dataset and predict the binary outcome. 
    The outcome has to be a binary outcome that is either zero or one or a yes or no. 
    In other words the output of a logistic regression is always categorical (= discrete)

- question: What do you predict with logistic regression?
  answer: >
    Logistic regression predicts the probabilty that a variable belongs to one 
    class or another. 

- question: Can you explain a real use case for logistic regression?
  answer: >
    You can use logistic regression to predict the probability that a customer will buy 
    a product.	

- question: How can you turn logistic regression into a classifier?
  answer: >
    To turn logistic regression into a classifier you can use a threshold. If the 
    output of the logistic regression is above (below) the threshold, then the output
    can be classified as class zero (class one).

- question: >
    What are some metrics that you can use to evaluate regression models?
  answer: >
    To evaluate regression models you can use a number of metrics including
    <ul>
    <li><b>\(R^2\)</b> (also known as coefficient of determination) represents how well the variables
    of the regression model describe the variance of the target variable.</li>
    <li><b>Mean absolute error (MAE)</b> is the average of the absolute distances between predictions and actual values.</li>
    <li><b>Mean squared error (MSE)</b> is the mean of the square of the absolute distances between predictions and actual 
    values. Notice that larger values, i.e. outliers, are amplified because the values are squared.</li>
    </ul>

- question: >
    To evaluate the performance of regression models you can use the R-squared (\(R^2\)) metric. 
    What is R-squared? How is it computed?
  answer: >
    Given a regression model, the <b>R-squared</b> (\(R^2\)) metric, also known as <b>coefficient
    of determination</b>, is a measure that represents how well the variables of the regression 
    model describe the variance of the target variable. 
    <br><br>
    \(R^2\) ranges from negative infinity to 1:
    <ul>
    <li>\(R^2\) = 1 indicates that all the variance of the target variable is explained by the independent
    variable(s)</li>
    <li>\(R^2\) > 0 indicates that a portion of the variance of the target variable is explained by the independent
    variable(s)</li>
    <li>\(R^2\) = 0 indicates that no information variance of the target variable is explained by the independent
    variable(s)</li>
    <li>\(R^2\) < 0 suggests that the independent variable(s) have no predictive power or may be 
    negatively correlated with the target variable.</li>
    </ul>
    <br><br>
    Given the real values \(yreal_i\) and the predicted values \(ypred_i\), 
    the formula of R-squared \(R^2\) is
    
    $$R^2 = 1 - \frac{\sum_{i=1}^{n} (yreal_i - ypred_i)^2}
    {\sum_{i=1}^{n} (yreal_i - \overline{yreal})^2} $$


- h3: K-Nearest neighbor algorithm (KNN)
  question: What is the K-nearest neighbor (KNN) algorithm? For what is KNN used?
  answer: >
    The <b>k-nearest neighbor (KNN)</b> algorithm is a machine learning algorithm that finds the 
    <code>K</code> closest (= most similar) data points of a given data point
    based on a certain metric, typically the Eucledian distance. 
    <br><br>
    The KNN algorithm can be used both for classification and regression. If you want to classify a data point, 
    you can assign assign it to the most common class among the <code>K</code> neighbors. 
    If you want to predict a value, you can compute the average value among the <code>K</code> neighbors. 

- question: What are some advantages and disadvantages of the KNN algorithm?
  answer: >
    Some advantages of the KNN algorithm are
    <ul>
    <li>simple concept</li>
    <li>builiding a model is cheap</li>
    <li>no assumption is made about the data distrubution</li>
    </ul>
    <br><br>Some disadvantages of the KNN algorithm are
    <ul>
    <li>classifying unknown records is expensive, in particular with big data sets, 
         since you have to find the K nearest neighbors</li>
    <li>pretty affected by data missing and data scaling</li>
    </ul>
- question: Make an example of a real user-case for the KNN algorithm
  answer: >
    You can use the <b>KNN algorithm</b> to recommend a product to a customer 
    based on the purchases of other similar customers.
    

- h3: Decision trees
  question: What is a decision tree?
  answer: >
    A <b>decision tree</b> is a supervised machine learning algorithm that generates a flowchart from a dataset. This flowchart
    can be represented as a tree-like structure. You can also think of a decision tree as a sequence of if-else conditions where a
    a feature is evaluated at each if-else condition.
    <br><br>
    For example, the decision tree in the picture below models the question of going to the beach. 
    According to this decision tree, we go to the beach only when the weather is sunny and the temperature is higher than 25 Â°C.
  image: decision_tree.PNG
- question: How are decision trees built?
  answer: >
    <b>Building a decision tree</b> is a recursive process. 
    At each step the algorithm searches for the best possible feature that can be used to split the dataset while minimizing the
    entropy within each subset. The subsets are then split repeatedly into even smaller subsets, and so on and so forth until 
    the process stops when the algorithm determines the data within the subsets are sufficiently homogenous, or another 
    stopping criterion has been met.
    <br><br>
    The <b>entropy</b> describes the variety in a dataset. An entropy of 0 means that all items are in the same class. The 
    maximum entropy possible is when each item is in a different class.  
    <br><br>This algorithm is a greedy algorithm since it chooses a local optimal solution at each step.
- question: What are the main advantages of decision trees?
  answer: >
    The <b>main advantages of decision trees</b> are:
    <ul>
    <li><b>Output simple to understand</b>: the output tree is human readable, i.e. it can be interpreted easily. 
    This makes decision trees particularly interesting for cases 
    where the decision mechanism needs to be transparent (for example in medical diagnostics).</li>
    <li><b>Minimum data preparation</b>: decision trees do require less data cleaning and data preparation. 
    Indeed they do not need data normalization/scaling and
    they can work well even with missing values.</li>
    </ul>
- question: What are some disadvantages of decision trees?
  answer: >
    Some <b>disadvantages of decision trees</b> are:
    <ul>
    <li><b>Overfitting</b>: decision trees can easily overfit the data.</li>
    <li><b>Instability</b>: small changes in the input data can lead to a very different tree output structure.</li>
    <li><b>Complex data structure</b>: the structure of decision trees can become complex for example when the dataset has a lot
    of features.</li>
    </ul>
- question: >
    What technique can you use to reduce the overfitting of a decision tree? How many approaches are there?
  answer: >
    To <b>reduce the overfitting of a decision tree</b> you can use <b>pruning</b>, which reduces the size of the tree. 
    There are two approaches to pruning<br>
    <ul>
    <li><b>Pre-pruning</b>: the tree stop growing when it reaches a certain number of decisions or when 
    the decision nodes contains too few items. A disadvantage of this approach is that some important 
    data could be pruned.</li>
    <li><b>Post-pruning</b>: the tree can grow as needed and only at the end it is checked if the tree 
    is too big. This approach guarantees that the important data is not pruned.</li>
    </ul>
- question: >
   What are the main techniques used to combine decision trees to obtain a more accurate model 
   (so called ensemble techniques)?
  answer: >
    There are two main ensemble techniques:
    <ul>
    <li><b>Boosting algorithms</b>: this is a sequential process. In boosting algorithms learners 
    are learned sequentially with early learners fitting simple models to the data and then analyzing 
    data for errors. After each training step the weights are redistributed. 
    Misclassified data gets an increased weight so that this data will be more on focus in the 
    next train step.</li>
    <li><b>Bagging techniques</b>: this is a parallel process. In a bagging technique the dataset is 
    divided into <code>n</code> samples using randomized sampling. Then a model is build on each sample. 
    After that the resulting models are combined using voting or averaging.</li>
    </ul>
    
- h3: Support Vector Machine
  question: What is Support Vector Machine and what is its main idea?
  answer: >
    <b>Support Vector Machine</b> (SVM) is a supervised machine learning algorithm that tries to find high dimensional
    planes that divide a dataset into clusters. The planes are found in such a way that they are as far as possible from any point.
    <br><br>
    The position of the planes depends on the so called <b>support vectors</b>, that are the points that are closest to the planes.
    
- question: What is the technique called <i>kerneling</i> that is used by Support Vector Machine?
  answer:
    Under the hood, SVM uses a technique called <b>kerneling</b> to find clusters that might not be apparent in lower
    dimensions. More specifically, the dataset is mapped via kernel functions into a higher dimension where the
    data is potentially easier to cluster.

- question: What is Support Vector Machine particulary good at? What are some disadvantages? 
  answer: >
      <b>Support Vector Machine (SVM)</b> is particularly good at classifying high-dimensional data, i.e. data with a lot of features.
      <br><br>
      SVM has some disadvantages like:
      <ul>
        <li>it does not perform well with large datasets because of high computational costs.</li>
        <li>the kernel must be chosen carefully and this can be tricky.</li>
      </ul> 

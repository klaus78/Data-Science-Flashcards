- h2: Machine Learning
  h3: Machine Learning basic concepts
  question: What is the difference between data science and machine learning?
  answer: >
    <b>Data science</b> and <b>Machine learning</b> are closely related fields but they differ
    in their goals.
    <br><br> 
    <b>Data science</b> is more focused on the gathering and analysis of data. Data science
    includes gathering data from different sources, cleaning the data and analyzing the data
    in order to extract useful information. 
    <br><br>
    <b>Machine learning</b> is more focused on creating a mathematical model from a dataset in order to 
    predict something. For example given a dataset of house prices, you would like to predict 
    how the house prices evolves in the next months based on the historical data in the dataset. With
    machine learning algorithms you can create mathematical models that can make such a prediction.

- question: What are the most common types of problems that machine learning can help solve?
  answer: >
    Machine learning can help solve a large number of problems which can be grouped into
    the following types:
    <ul>
    <li><b>Classification</b>: the goal of classification is to predict the category of an input item. Examples
    of classification are email spam detection and desease diagnosis.</li>
    <li><b>Regression</b>: the goal of regression is the prediction of a numerical value. Examples of
    regression are predicting the price of houses or the future demand for products.</li>
    <li><b>Clustering</b>: the goal of clustering is to group items in a way that similar items are within
    the same group. Examples of clustering are grouping customers based on purchasing behavior or grouping documents by topic.</li>
    <li><b>Recommendation</b>: the goal of recommendation is to provide personalized suggestions. Examples
    of recommendation are movie recommendation and personal marketing based on personal preferencies.</li>
    <li><b>Anomality detection</b>: the goal of anomality detection is to identify unusual patterns in the data.
    Examples of anomality detection are credit fraud detection and intrusion detection in computer networks.</li>
    <li><b>Time series analysis</b>: the goal of time series analysis is to analyze data points collected at
    specific time intervals. Examples of time series analysis are energy consumption prediction and weather forecasting.</li>
    </ul>

- question: What are the typical steps of the creation of a machine learning model?
  answer: > 
    The typical steps of a machine learning process are:
    <ul>
    <li><b>Problem definition and domain knowledge</b>: understand what problem you want to solve and gain 
    domain knowledge related to that problem.
    </li>
    <li><b>Data preparation</b>: gather, clean the data (es. handling missing values and outliers) and perform feature engineering 
    (es. select features or create new ones to be used in a machine learning algorithm).</li>
    <li><b>Data splitting</b>: split the dataset into a training dataset and a test dataset. The machine learning model
    will be trained on the training dataset and validated on the test dataset.</li>
    <li><b>Model selection</b>: select an appropriate machine learning algorithm depending on the problem at hand.</li>
    <li><b>Training</b>: train the selected machine learning algorithm on the training dataset. During the training
    the machine learning algorithm learns structures and relationships contained in the training data.</li>
    <li><b>Evaluation</b>: evaluate the performance of the trained model using the test dataset. The model
    makes predictions using the items in the test dataset and the predicted values are compared with the real values.</li>
    <li><b>Optimization</b>: adjust and tweak the trained model to improve the performance.</li>
    <li><b>Deployment</b>: the machine learning model is put into production where it can be used to make
    predictions using new data.</li>
    <li><b>Monitoring performance</b>: monitor the performance of the model over time and collect feedback. If necessary
    retrain and update the model.</li>
    </ul>

  question: What is bias in machine learning?
  answer: >
    <b>Bias</b> in machine learning is a phenomenon that occurs when a model produces results that are consistently unfair 
    or inaccurate for certain groups of people due to erroneous assumptions in the machine learning (ML) process. 
    <br><br>
    Bias can happen for a number of reasons:
    <ul>
    <li>
      <b>data bias</b>: data bias can occur when the training data fails to adequately capture the diversity 
      and complexity present in the real-world data for a specific problem.
    </li>
    <li><b>algorithms bias</b>: an algorithm itself can be biased, either due to the way it is trained or designed.
    For example, it can occur that an algorithm inadvertently learn biased patterns in the data. This can happen if
    an algorithm is not carefully designed to account for bias in the training data.
    </li>
    </ul>

- question: What does high bias mean for a machine learning model?
  answer: >
    In machine learning, <b>high bias</b> refers to a situation where a model has a strong and often simplistic bias or 
    assumption about the underlying data. 
    <br><br>
    High bias can lead to the model underfitting the data, meaning it fails to capture 
    the true underlying patterns and relationships in the data. In other words, the model is too simple or constrained to represent 
    the complexity of the data, resulting in poor performance.

- question: What is the variance of a machine learning model?
  answer: > 
    <b>Variance</b> in machine learning is the amount by which a model's predictions change when it is trained 
    on different subsets of the training data.
    <br><br>
    In other words, variance measures how much the model overfits the training data. A model with high variance
    will work well with the training data but not generalize well to the test data.
    <br><br>
    A model with low variance will be less sensitive to the training data and will perform well also with the test data. 
    
- question: What are the implications of high variance in a machine learning model?
  answer: > 
    A machine learning with <b>high variance</b> is a model for which small changes in the training data set lead to big changes 
    in the model prediction. This is equivalent to saying that the model overfits the training data, i.e. the model works well 
    on the training data but does not generalize well on the test data.
    <br><br>
    In general, high variance in a machine learning model can lead to poor generalization performance, meaning that the model will 
    not perform well on new data that it has not seen before.

- question: Explain the bias-variance trade-off in the context of model complexity.
  answer: >
      The <b>bias-variance trade-off</b> is a fundamental concept in machine learning that deals with the relationship between model 
      complexity and its performance:
      <ul>
      <li><i>High Bias (Underfitting)</i>: When a model is too simple or has too few parameters to capture the underlying patterns in the data, it exhibits high bias. 
      This leads to underfitting, where the model performs poorly on both training and test data.</li>

      <li><i>High Variance (Overfitting)</i>: When a model is overly complex or has too many parameters, it becomes highly sensitive to the 
      noise and random variations in the training data. This results in high variance, leading to overfitting, where the model 
      performs well on training data but poorly on new, unseen data.</li>
      </ul>
      Achieving a balance between bias and variance is crucial. Regularization techniques, cross-validation, and hyperparameter tuning can help manage 
      the bias-variance trade-off.

- question: Explain overfitting and underfitting in the context of machine learning models.
  answer: >
    <b>Overfitting</b> occurs when a machine learning model performs very well on the training data but 
    poorly on new, unseen data. Overfitting happens when the model is too complex, capturing noise and random fluctuations 
    in the training data, rather than the underlying patterns. 
    <br><br>
    <b>Underfitting</b>, on the other hand, happens when a model is too simple to capture the underlying patterns in the data. 
    It performs poorly both on the training data and unseen data. It is characterized by high bias and low variance.

- question: What are some techniques that you can use to prevent overfitting in machine learning?
  answer: >
    <b>Overfitting</b> happens when your model works well with the training dataset but
    does not work well with new data, which means the model cannot generalize well. 
    <br><br>
    To prevent overfitting you can for example use regularization and cross-validation.

- question: What is regularization?
  answer: >
    <b>Regularization</b> is a technique used to prevent overfitting in machine learning models. Regularization involves adding a 
    penalty term to the loss function during training, which discourages the model from assigning too much importance 
    to any particular feature. Common forms of regularization include L1 (Lasso) and L2 (Ridge) regularization.
    <br><br> 
    Regularization helps in reducing the complexity of the model, leading to better generalization to unseen data. 
    It is controlled by a regularization parameter, which determines the trade-off between fitting the training data 
    well and keeping the model's complexity in check.

- question: What is the curse of dimensionality?
  answer: >
    The <b>curse of dimensionality</b> is a tendency that it is easier to overfit a dataset 
    when there are few points and many features. Data needs to increase exponentially 
    with the number of features in order not to have overfitting.

- question: What is supervised learning?
  answer: >
    <b>Supervised learning</b> is a machine learning process where the learning algorithms are trained on
    labeled datasets, which means that each point in the dataset is associated with a class or label.
    <br><br>
    The primary goal of supervised learning is to create a mathematical model that maps input data to the corresponding output 
    or target variable, so that the model can make accurate predictions or classifications 
    when given new, unseen input data.

- question: What is unsupervised learning? Make also some examples of algorithms
  answer: > 
    <b>Unsupervised learning</b> is a machine learning process with unlabeled data.
    <br><br>
    Some examples of unsupervised learning are
    <br> - Clustering algorithms (K-means, hierarchical custering, Probabilistic clustering)
    <br> - Dimension reduction algorithms (PCA, Single Value Decomposition SVD)

- question: What is the difference between supervised and unsupervised learning?
  answer: |
      <b>Supervised learning</b> is a type of machine learning where the algorithm is trained on a labeled dataset, meaning 
      that the input data is paired with corresponding output labels. The goal of supervised learning is to learn a mapping 
      from inputs to outputs, making it suitable for tasks like classification and regression.
      <br><br>
      <b>Unsupervised learning</b>, on the other hand, deals with unlabeled data. The algorithm tries to find patterns or 
      structure in the data without the guidance of labeled output. It includes tasks like clustering and dimensionality reduction.

- question: What are the two main types of supervised learning algorithms?
  answer: >
    Supervised machine learning algorithms are mainly classified 
    into classification and regression.

- question: Can you mention some supervised machine learning algorithms?
  answer: >
    The most common <b>supervised machine learning algorithms</b> are 
    <br>K-nearest neighbor, Naive Bayes, Decision Trees, Linear Regression, 
    Support Vector Machines.
  
- question: What is reinforcement learning?
  answer: >
    <b>Reinforcement learning</b> is a machine learning algorithm that receives feedback on its output so that the accuracy
    of the output is improved based on this feedback. In other words the algorithm learns through trial
    and error.

- question: What are hyperparameters?
  answer: >
    <b>Hyperparameters</b> are parameters that are passed to machine learning models to control the learning process. 
    The hyperparameters are set before starting the training model and their selection plays a critical role
    in achieving optimal results.
    <br><br>
    Examples of hyperparameters are the <code>k</code> value in the k-nearest neighbor algorithm or the number of nodes 
    in a neural network.

- question: What are the main differences between parameters and hyperparameters?
  answer: >
    <b>Parameters</b> and <b>hyperparameters</b> are crucial in machine learning models, but 
    they play different roles. Here are the key differences between them:
    <br><br>
    <b>Parameters</b> are internal variables within machine learning models. They are computed
    automatically by the machine learning algorithms during the training process.
    The purpose of parameters is to fine-tune the model's performance based on the training data, 
    with the goal of minimizing the model error or loss function.  
    <br><br>
    <b>Hyperparameters</b> are passed to machine learning algorithms before the training process begins.
    Hyperparameters influence how the algorithms learn and generalize from data. 
    Data scientists or machine learning engineers set these values, often utilizing various 
    hyperparameter tuning techniques to optimize the model's performance.

- question: What are ensemble methods in machine learning?
  answer: >
    <b>Ensemble methods</b> are techniques that combine the results of a number of machine learning models rather
    than using a single model. The basic idea behind ensemble methods is than the combination of the results of a number of models 
    should be more accurate than using a single model.
    <br><br>
    To provide an example in real life, ensemble methods can be compared to asking several doctors for a diagnosis 
    rather than relying on a single doctor. In the former case, one would expect the diagnosis to be more accurate.

    
- h3: Evaluate machine learning methods
  question: To evaluate a generic machine learning model what metrics do you typically use?
  answer: > 
    The metrics to use for the evaluation of a machine learning model depend on the type of problem
    we are trying to solve.



    You would probably use the misclassification error or accuracy. 
    However, notice that accuracy works well when all classes are equally important and the 
    training dataset is balanced.	

  question: What is cross-validation in machine learning?
  answer: > 
    <b>Cross-validation</b> is a technique used to assess the performance and generalization 
    of a machine learning model. It involves splitting the dataset into multiple subsets, typically a 
    training set and a validation set, multiple times. The model is trained and evaluated on different 
    subsets to get a more accurate estimate of its performance.
    <br><br>
    Cross-validation is essential because it helps in:
    <ul>
      <li>Detecting overfitting or underfitting.</li>
      <li>Providing a more reliable estimate of a model's performance.</li>
      <li>Ensuring that the model can generalize well to new, unseen data.</li>
    </ul>

- question: What is K-fold cross-validation?
  answer: >
    <b>K-fold cross-validation</b> is a specific type of cross-validation where the dataset is split into <i>K</i> equal parts. 
    <br>The steps of <b>K-fold cross-validation</b> are:
    <ul> 
    <li>split a dataset into <i>K</i> equal parts. Each part is called fold.</li>
    <li>take one fold <i>i</i> as the test set and take the remaining <i>(K-1)</i> folds as the training set.</li>
    <li>train you classification algorithm on the training set and evaluate its performance of the test set.</li> 
    <li>Repeat the process for each fold. As a result, for each fold we get an accuracy value and 
    in total we get <i>K</i> accuracy values.</li>
    <li>the final accuracy value for your classification algorithm is given by the average of
    the <i>K</i> accuracy values.</li>
    </ul>

- question: What matrix can you use to evaluate the performance of a classifier? How does an ideal matrix look like?
  answer: >
    Let's suppose we have a binary classifier that can assign an item to two
    possible classes, which are typically labeled as positive (1) and negative (0). 
    <br><br>
    To evaluate the performance of the classifier you can use the <b>confusion matrix</b>. 
    The confusion matrix displays the number of true positives (TP), true negatives (TN),
    false positives (FP) and false negatives (FN). 
    <br>
    <ul>
    <li>True positives (TP) = number of items with label positive (1) that 
    are classified correctly as positive.</li>
    <li>True negatives (TN) = number of items with label negative (0) that 
    are classified correctly as negative.</li>
    <li>False positives (FP) = number of items with label negative (0) that
    are classified wrongly as positive.</li>
    <li>False negatives (FN) = number of items with label positive (1) that
    are classified wrongly as negative.</li>
    </ul>
    <br>
    The confusion matrix looks like

    <br><br>
    <table>
      <tr>
        <td></td>
        <th><b>actual 1</b></th> 
        <th><b>actual 0</b></th>
      </tr>
      <tr>
        <td><b>predicted 1</b></td>
        <td class="true-positive">TP</td> 
        <td class="false-positive">FP</td>
      </tr>
      <tr>
        <td><b>predicted 0</b></td>
        <td class="false-negative">FN</td> 
        <td class="true-negative">TN</td>
      </tr>
    </table>
    <br><br>
    An ideal confusion matrix has high values of TP and TN because these quantities indicate the correctly
    classified items.

- question: Why is the confusion matrix called like that?
  answer: >
    The <b>confusion matrix</b> is a table that is used to evaluate the performance of a classifier.
    It displays the number of correctly classified items (true positives and true negatives) and
    wrongly classified items (false positives and false negatives).
    <br><br>
    The <b>confusion matrix</b> is called like that because it helps you identify where the 
    classifier has made wrong predictions, which is where the classifier was <b><i>confused</i></b>.
    As a result you can gain insight about the performance of the classifier and take corrective
    action to minimize the number of wrongly classified items.
    

- question: Can you mention some of the most common metrics used to evaluate binary classifiers?
  answer: >
    A binary classifier is a machine learning model that assigns an item to one of two possible classes.
    The two classes are typically referred to as the positive class and the negative class. 
    <br>
    In order to evaluate the performance of binary classifiers, the following terms are used: 
    <ul>
    <li>True positives (TP) = number of the positive class items that 
    are classified correctly as positive.</li>
    <li>True negatives (TN) = number of the negative class items that 
    are classified correctly as negative.</li>
    <li>False positives (FP) = number the negative class items that
    are classified wrongly as positive.</li>
    <li>False negatives (FN) = number of the positive class items that are classified 
    wrongly as negative.</li>
    </ul>
    <br>
    Now that we have defined TP, TN, FP and FN and given <code>n</code> as the total number of items,
    we can introduce some of the most common metrics for evaluating binary classifiers. 
    <ul>
    <li><b>Accuracy</b> is the percentage of the correctly classified items. 
    <br>Accuracy = 
      <div class="frac">
        <span>TP + TN</span>
        <span class="symbol">/</span>
        <span class="bottom">n</span>
      </div>
    </li>
    <li><b>true positive rate or sensitivity  or recall</b> is the percentage of the positive class items that
    are actually classified as positive.
    <br>Sensitivity = 
      <div class="frac">
        <span>TP</span>
        <span class="symbol">/</span>
        <span class="bottom">TP + FN</span>
      </div>
    </li>
    <li><b>true negative rate or specificity</b> is the percentage of the negative class items that
    are actually classified as negative. 
    <br>Specificity = 
      <div class="frac">
        <span>TN</span>
        <span class="symbol">/</span>
        <span class="bottom">TN + FP</span>
      </div>
    </li>
    <li><b>Precision</b> is the percentage of the items classified as positive that actually belong to the positive class. 
    <br>Precision = 
      <div class="frac">
        <span>TP</span>
        <span class="symbol">/</span>
        <span class="bottom">TP + FP</span>
      </div>
    </li>
    <li><b>False positive rate (FPR)</b> represents the proportion of negative class items 
    that are incorrectly classified as positive. 
      <br>FPR = 
      <div class="frac">
        <span>FP</span>
        <span class="symbol">/</span>
        <span class="bottom">TN + FP</span>
      </div>
    <li><b>False negative rate (FNR)</b> represents the proportion of positive class items 
    that are incorrectly classified as negative.
      <br>FNR = 
      <div class="frac">
        <span>TN</span>
        <span class="symbol">/</span>
        <span class="bottom">TP + FN</span>
      </div>
    </li>
    <ul>

- question: What is the difference between precision and recall in binary classification? How are they related?
  answer: >
    <b>Precision</b> and <b>recall</b> are two important metrics in binary classification. 
    <ul>
    <li><b>Precision</b> measures the proportion of 
    true positive predictions among all positive predictions made by the model. 
    <br>
    Precision is calculated as 
    <div class="frac">
        <span>TP</span>
        <span class="symbol">/</span>
        <span class="bottom">TP + FP</span>
      </div>, 
    where TP is true positives, and FP is false positives.</li>
    <li><b>Recall</b> (Sensitivity) measures the proportion of true positive 
    predictions among all actual positive instances.
    <br> 
    Recall is calculated as 
    <div class="frac">
        <span>TP</span>
        <span class="symbol">/</span>
        <span class="bottom">TP + FN</span>
      </div>, where FN is false negatives. 
    </li>
    </ul>
    Precision and recall are related through a trade-off. Increasing one often leads to a decrease in the other. 
    A model with high precision makes fewer false positive errors but may miss some true positives. Conversely, a model 
    with high recall captures more true positives but may produce more false positives.

- question: > 
    What metrics are more meaningful with class-balanced datasets and what work better 
    for class-unbalanced datasets? 
  answer: >
    In a class-balanced dataset, the number of items in each class is approximately the same.
    
    <br><br>When the dataset is class-balanced, <b>accuracy</b> is a good metric. Indeed, accuracy assigns
    equal weight to both true positives and true negatives. Accuracy is calculated as the ratio of correctly 
    classified instances (both true positives and true negatives) to the total number of instances, 
    as indicated by the formula
    <br>Accuracy = 
      <div class="frac">
        <span>TP + TN</span>
        <span class="symbol">/</span>
        <span class="bottom">n</span>
      </div>
    
    <br><br>When the dataset is class-unbalanced, one class has significantly more items than the
    other classes. In that case <b>precision</b> and <b>recall</b>
    are better metrices. In class-imbalanced scenarios, 
    the class with fewer instances is often the class of more significance. 
    For example, in fraud detection, the number of actual fraud cases is much lower than non-fraud cases, 
    but correctly identifying fraud is crucial. 
    <br>Let TP represent the number of correctly identified frauds. 
    Precision and recall provide a better indication of the model's ability to 
    identify the minority class accurately, as they have TP in the numerator as follows
    <br>Precision = 
      <div class="frac">
        <span>TP</span>
        <span class="symbol">/</span>
        <span class="bottom">TP + FP</span>
      </div> 
    <br>Recall = 
      <div class="frac">
        <span>TP</span>
        <span class="symbol">/</span>
        <span class="bottom">TP + FN</span>
      </div>
    <br><br>By focusing on the true positives, precision and recall provide 
    insights into how well the model performs in the context that matters most.
    <br><br>
    Precision: High precision indicates that when the model predicts the positive class, it is often correct. 
    This is important when misclassifying positive instances (false positives) could have significant consequences. 
    Precision tells you the proportion of correctly identified positive predictions out of all predicted positive instances.
    <br><br>
    Recall: High recall indicates that the model is effective at capturing most of the actual positive instances. 
    This is crucial when you want to make sure you don't miss many of the positive instances. 
    Recall tells you the proportion of correctly identified positive predictions out of all actual positive instances.
    <br><br>
    However, it's important to remember that precision and recall are trade-offs: increasing one can sometimes 
    lead to a decrease in the other. Therefore, you should choose the metric that aligns with your specific goals 
    and priorities in your application.

- question: > 
    To evaluate machine learning models for cancer detection what metrics do 
    doctors typically use?
  answer: > 
    Machine learning models for <b>cancer detection</b> aim to classify patients as having cancer (positive)
    or healthy (negative). In cancer detection it is important to keep in mind that the datasets
    are typically strongly unbalanced, with few cancer samples and many healthy samples.
    <br><br>
    Because of this, it is evident that accuracy cannot be used since it requires balanced datasets.
    The typical metrics used for cancer detection are rather:
    <ul>
      <li><b>Sensitivity</b>: sensitivity measures the ability of the model to 
      correctly identify patients who have cancer among all the patients who actually have cancer.
      A high sensitivity indicates that the model is good at catching cancer cases.</li>
      <li><b>Specificity</b>: specificity measures the ability of the model to correctly identify patients who do 
      not have cancer among all the patients who do not have cancer. A high specificity indicates that the model is 
      good at avoiding false alarms for patients without cancer.</li>
    </ul> 
    <br>
    The choice of what metric to use should be tailored to the specific clinical context, as different cancer types and 
    clinical scenarios may have varying requirements.
 
- question: What is the meaning and use of the F-measure (also called F1-score)?
  answer: >
    The <b>F-measure</b>, also known as the F1-score, is a metric for the evaluation of binary classifiers that combines 
    precision and recall using the harmonic mean.
    <br><br> F-measure = 
    <div class="frac">
      <span>2 * (Precision * Recall)</span>
      <span class="symbol">/</span>
      <span class="bottom">(Precision + Recall)</span>
    </div>
    <br><br>
    The F-measure value ranges between 0 (worst case) and 1 (best case). The F-measure can be used
    when you want to evaluate the performance of a binary classifier, taking into account both precision and recall.
    A high F-measure value suggests a good balance between precision and recall, while a low F-measure value
    indicates that either precision or recall might be low, highlighting potential issues with the classifier's performance.

- question: If you want to evaluate the performance of a classifier with a curve what do you use?
  answer: >
    To evaluate the performance of a classifier you can use the <b>ROC curve</b> 
    (Receiver Operating Characteristic). The ROC curve is a graphical 
    representation of the performance of a binary classification model for different values of a threshold that affects the 
    classification performance.
    The ROC curve illustrates the trade-off between the classifier's sensitivity (True Positive Rate) and its specificity 
    (the complement of True Negative Rate, often referred to as False Positive Rate) as you vary that threshold.
    <br><br>
    Sensitivity = probability of predicting that a real positive will be positive.
    <br>
    Specificity = probability of predicting that a real negative will be negative.
    <br><br>
    The goal when assessing a classifier's performance is to maximize sensitivity while minimizing the False 
    Positive Rate (1-specificity). In other words, a high sensitivity and low False Positive Rate are desirable.
    <br><br>
    The ROC curve is a graphical representation of this trade-off. It is a plot of <code>sensitivity</code> against <code>1-specificity</code> 
    for different threshold values. The closer the ROC curve is to the top-left corner (point <code>[0, 1]</code>), the better the classifier's 
    performance. Alternatively, the farther the curve is from the diagonal line connecting points <code>[0, 0]</code> and <code>[1, 1]</code>, 
    the better the classifier performs. This diagonal line represents the expected performance of a random classifier.
    <br><br>
    To quantify the overall performance, the area under the ROC curve (AUC) is calculated. A perfect classifier would have an AUC 
    of 1, indicating perfect discrimination. A random classifier would have an AUC of 0.5, corresponding to the diagonal line. 
    Therefore, a higher AUC value indicates better classifier performance, with a value close to 1 being the ideal outcome."
  image: roc_curve.png

- h3: Naive Bayes
  question: What is Naive Bayes?
  answer: >
    Naive Bayes is a supervised machine learning algorithm based on the Bayes theorem that is used
    to solve classification problems.

- question: Why is Naive Bayes called "naive"?
  answer: >
    Because the Naive Bayes is based on the idea that the predictor variables are independent of 
    each other while in nature this is often not the case.
    <br><br>
    A Naive Bayes classifier calculates the probability of an item of belonging to each possible
    output class. The class with the highest probability is then chosen as output.

- question: What is the Bayes theorem?
  answer: >
    The Bayes theorem is a mathematical formula for finding P(A|B) from P(B|A).
    The formula is 
    <br><br>
    P(A|B) = P(B|A) * P(A) / P(B)


- question: What are the main advantages of Naive Bayes?
  answer: >
    The Naive Bayes is a relatively simple and very quick algorithm since it is a probabilistic 
    model that does not require a training step. This makes it very scalable.

- question: When is the Naive Bayes usually used?
  answer: > 
    Naive Bayes is often used in <br>
    - text classification like spam filter<br>
    - real-time classification since it is fast<br>
    - multi-class prediction

- question: When Naive Bayes is used with numerical variables, what condition is assumed on the data?
  answer: >
    Numerical variables used in Naive Bayes are expected to have a normal distribution.
    This is not always the case in practice.

- question: How does the Naive Bayes perform with categorical variables?
  answer: > 
    It performs well with categorical variables since no assumpion is made on
    the data distribution. By contrast with numerical variables a normal distribution
    is assumed.

- h3: Regression
  question: What do you predict with linear regression?
  answer: Linear regression predicts a real value

- question: What are the most common techniques used for computing the coefficients in linear regression?
  answer: >
    The most common techniques used for computing the coefficients in linear regression are 
    <br><br> - <b>Ordinary Least Squares</b> seeks to minimize the sum of the squared residuals, i.e. the distance 
    between the points and the regression line.
    <br><br> - <b>Gradient descendent</b> works by inizializing randomly the coefficients. A learning rate is 
    used as a scale factor and the coefficients are updated in the direction towards minimizing the error. 
    The process is repeated until a minimum sum squared error is achieved or no further improvement is 
    possible.

- question: What is logistic regression?
  answer: >
    <b>Logistic regression</b> is a statistical technique that is used to analyze a dataset and predict the binary outcome. 
    The outcome has to be a binary outcome that is either zero or one or a yes or no. 
    In other words the output of a logistic regression is always categorical (= discrete)

- question: What do you predict with logistic regression?
  answer: >
    Logistic regression predicts the probabilty that a variable belongs to one 
    class or another. 

- question: Can you explain a real use case for logistic regression?
  answer: >
    You can use logistic regression to predict the probability that a customer will buy 
    a product.	

- question: How can you turn logistic regression into a classifier?
  answer: >
    To turn logistic regression into a classifier you can use a threshold. If the 
    output of the logistic regression is above (below) the threshold, then the output
    can be classified as class zero (class one).

- question: >
    What are some metrics that you can use to evaluate regression models?
  answer: >
    To evaluate regression models you can use a number of metrics including
    <ul>
    <li><b>\(R^2\)</b> (also known as coefficient of determination) represents how well the variables
    of the regression model describe the variance of the target variable.</li>
    <li><b>Mean absolute error (MAE)</b> is the average of the absolute distances between predictions and actual values.</li>
    <li><b>Mean squared error (MSE)</b> is the mean of the square of the absolute distances between predictions and actual 
    values. Notice that larger values, i.e. outliers, are amplified because the values are squared.</li>
    </ul>

- question: >
    To evaluate the performance of regression models you can use the R-squared (\(R^2\)) metric. 
    What is R-squared? How is it computed?
  answer: >
    Given a regression model, the <b>R-squared</b> (\(R^2\)) metric, also known as <b>coefficient
    of determination</b>, is a measure that represents how well the variables of the regression 
    model describe the variance of the target variable. 
    <br><br>
    \(R^2\) ranges from negative infinity to 1:
    <ul>
    <li>\(R^2\) = 1 indicates that all the variance of the target variable is explained by the independent
    variable(s)</li>
    <li>\(R^2\) > 0 indicates that a portion of the variance of the target variable is explained by the independent
    variable(s)</li>
    <li>\(R^2\) = 0 indicates that no information variance of the target variable is explained by the independent
    variable(s)</li>
    <li>\(R^2\) < 0 suggests that the independent variable(s) have no predictive power or may be 
    negatively correlated with the target variable.</li>
    </ul>
    <br><br>
    Given the real values \(yreal_i\) and the predicted values \(ypred_i\), 
    the formula of R-squared \(R^2\) is
    
    $$R^2 = 1 - \frac{\sum_{i=1}^{n} (yreal_i - ypred_i)^2}
    {\sum_{i=1}^{n} (yreal_i - \overline{yreal})^2} $$


- h3: K-Nearest neighbor algorithm (KNN)
  question: What is the K-nearest neighbor (KNN) algorithm? For what is KNN used?
  answer: >
    The <b>k-nearest neighbor (KNN)</b> algorithm is a machine learning algorithm that finds the 
    <code>K</code> closest (= most similar) data points of a given data point
    based on a certain metric, typically the Eucledian distance. 
    <br><br>
    The KNN algorithm can be used both for classification and regression. If you want to classify a data point, 
    you can assign assign it to the most common class among the <code>K</code> neighbors. 
    If you want to predict a value, you can compute the average value among the <code>K</code> neighbors. 

- question: What are some advantages and disadvantages of the KNN algorithm?
  answer: >
    Some advantages of the KNN algorithm are
    <ul>
    <li>simple concept</li>
    <li>builiding a model is cheap</li>
    <li>no assumption is made about the data distrubution</li>
    </ul>
    <br><br>Some disadvantages of the KNN algorithm are
    <ul>
    <li>classifying unknown records is expensive, in particular with big data sets, 
         since you have to find the K nearest neighbors</li>
    <li>pretty affected by data missing and data scaling</li>
    </ul>

- question: Make an example of a real user-case for the KNN algorithm
  answer: >
    You can use the <b>KNN algorithm</b> to recommend a product to a customer 
    based on the purchases of other similar customers.

- question: "Why does KNN work better on smaller datasets?"
  answer: >
    KNN (k-nearest neighbors) tends to perform better on smaller datasets due to several factors:

    1. **Simplicity and No Training Phase:**
       KNN does not involve a complex training phase where it learns intricate patterns from the data. Instead, it memorizes the entire dataset. This simplicity is advantageous when dealing with smaller datasets, making it easier to manage and interpret the model's behavior.

    2. **Non-Parametric Nature:**
       KNN is a non-parametric algorithm, meaning it doesn't assume a specific data distribution. It can capture complex and nonlinear relationships within the data, making it adaptable to the irregular patterns often found in smaller datasets.

    3. **Flexibility in Capturing Patterns:**
       Small datasets might exhibit diverse and intricate patterns. KNN's flexibility in capturing these patterns without imposing strong assumptions makes it a suitable choice. It considers the local structure of the data, which can be essential when dealing with limited data points.

    4. **Ease of Interpretation and Manual Inspection:**
       KNN's predictions are based on the actual data points in the vicinity of a query point. This makes it transparent and interpretable, allowing domain experts to manually inspect and verify the predictions. In situations where domain knowledge is crucial, this interpretability can be valuable.

    5. **Few Hyperparameters:**
       KNN has relatively few hyperparameters, with 'k' (the number of neighbors) being the primary one. The simplicity of the model and the limited number of parameters make it easier to fine-tune and experiment with different values of 'k' on smaller datasets.

    6. **Robustness to Noise:**
       Small datasets might contain noise and outliers. KNN can handle these situations reasonably well, especially when noise removal and data cleaning techniques are applied. With careful preprocessing, KNN can be made robust to noise in smaller datasets.

    Despite its advantages, it's essential to note that the performance of KNN heavily depends on choosing an appropriate 'k' value, selecting a suitable distance metric, and handling potential outliers. Careful experimentation and validation are necessary to ensure its effectiveness on any dataset size."

   
- h3: Decision trees
  question: What is a decision tree?
  answer: >
    A <b>decision tree</b> is a supervised machine learning algorithm that generates a flowchart from a dataset. This flowchart
    can be represented as a tree-like structure. You can also think of a decision tree as a sequence of if-else conditions where a
    a feature is evaluated at each if-else condition.
    <br><br>
    For example, the decision tree in the picture below models the question of going to the beach. 
    According to this decision tree, we go to the beach only when the weather is sunny and the temperature is higher than 25 Â°C.
  image: decision_tree.PNG
- question: How are decision trees built?
  answer: >
    <b>Building a decision tree</b> is a recursive process. 
    At each step the algorithm searches for the best possible feature that can be used to split the dataset while minimizing the
    entropy within each subset. The subsets are then split repeatedly into even smaller subsets, and so on and so forth until 
    the process stops when the algorithm determines the data within the subsets are sufficiently homogenous, or another 
    stopping criterion has been met.
    <br><br>
    The <b>entropy</b> describes the variety in a dataset. An entropy of 0 means that all items are in the same class. The 
    maximum entropy possible is when each item is in a different class.  
    <br><br>This algorithm is a greedy algorithm since it chooses a local optimal solution at each step.
- question: What are the main advantages of decision trees?
  answer: >
    The <b>main advantages of decision trees</b> are:
    <ul>
    <li><b>Output simple to understand</b>: the output tree is human readable, i.e. it can be interpreted easily. 
    This makes decision trees particularly interesting for cases 
    where the decision mechanism needs to be transparent (for example in medical diagnostics).</li>
    <li><b>Minimum data preparation</b>: decision trees do require less data cleaning and data preparation. 
    Indeed they do not need data normalization/scaling and
    they can work well even with missing values.</li>
    </ul>
- question: What are some disadvantages of decision trees?
  answer: >
    Some <b>disadvantages of decision trees</b> are:
    <ul>
    <li><b>Overfitting</b>: decision trees can easily overfit the data.</li>
    <li><b>Instability</b>: small changes in the input data can lead to a very different tree output structure.</li>
    <li><b>Complex data structure</b>: the structure of decision trees can become complex for example when the dataset has a lot
    of features.</li>
    </ul>
- question: >
    What technique can you use to reduce the overfitting of a decision tree? How many approaches are there?
  answer: >
    To <b>reduce the overfitting of a decision tree</b> you can use <b>pruning</b>, which reduces the size of the tree. 
    There are two approaches to pruning<br>
    <ul>
    <li><b>Pre-pruning</b>: the tree stop growing when it reaches a certain number of decisions or when 
    the decision nodes contains too few items. A disadvantage of this approach is that some important 
    data could be pruned.</li>
    <li><b>Post-pruning</b>: the tree can grow as needed and only at the end it is checked if the tree 
    is too big. This approach guarantees that the important data is not pruned.</li>
    </ul>
- question: >
   What are the main techniques used to combine decision trees to obtain a more accurate model 
   (so called ensemble techniques)?
  answer: >
    There are two main ensemble techniques:
    <ul>
    <li><b>Boosting algorithms</b>: this is a sequential process. In boosting algorithms learners 
    are learned sequentially with early learners fitting simple models to the data and then analyzing 
    data for errors. After each training step the weights are redistributed. 
    Misclassified data gets an increased weight so that this data will be more on focus in the 
    next train step.</li>
    <li><b>Bagging techniques</b>: this is a parallel process. In a bagging technique the dataset is 
    divided into <code>n</code> samples using randomized sampling. Then a model is build on each sample. 
    After that the resulting models are combined using voting or averaging.</li>
    </ul>
    
- h3: Support Vector Machine
  question: What is Support Vector Machine and what is its main idea?
  answer: >
    <b>Support Vector Machine</b> (SVM) is a supervised machine learning algorithm that tries to find high dimensional
    planes that divide a dataset into clusters. The planes are found in such a way that they are as far as possible from any point.
    <br><br>
    The position of the planes depends on the so called <b>support vectors</b>, that are the points that are closest to the planes.
    
- question: What is the technique called <i>kerneling</i> that is used by Support Vector Machine?
  answer:
    Under the hood, SVM uses a technique called <b>kerneling</b> to find clusters that might not be apparent in lower
    dimensions. More specifically, the dataset is mapped via kernel functions into a higher dimension where the
    data is potentially easier to cluster.

- question: What is Support Vector Machine particulary good at? What are some disadvantages? 
  answer: >
      <b>Support Vector Machine (SVM)</b> is particularly good at classifying high-dimensional data, i.e. data with a lot of features.
      <br><br>
      SVM has some disadvantages like:
      <ul>
        <li>it does not perform well with large datasets because of high computational costs.</li>
        <li>the kernel must be chosen carefully and this can be tricky.</li>
      </ul>